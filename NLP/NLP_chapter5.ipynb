{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 디코딩 : 모델의 확률 출력을 텍스트로 변환하는 방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 디코딩은 반복적으로 수행되므로 입력이 모델의 정방향 패스를 한 번 통과할 때마닫 많은 계산이 필요\n",
    "- 생성된 텍스트의 품질과 다양성은 디코딩 방법과 하이퍼 파라미터에 따라 달라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name =\"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (9.76%)</td>\n",
       "      <td>same (2.94%)</td>\n",
       "      <td>only (2.87%)</td>\n",
       "      <td>best (2.38%)</td>\n",
       "      <td>first (1.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>common (22.90%)</td>\n",
       "      <td>powerful (6.88%)</td>\n",
       "      <td>important (6.32%)</td>\n",
       "      <td>popular (3.95%)</td>\n",
       "      <td>commonly (2.14%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most common</td>\n",
       "      <td>type (15.06%)</td>\n",
       "      <td>types (3.31%)</td>\n",
       "      <td>form (1.91%)</td>\n",
       "      <td>way (1.89%)</td>\n",
       "      <td>and (1.49%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most common type</td>\n",
       "      <td>of (83.13%)</td>\n",
       "      <td>in (3.16%)</td>\n",
       "      <td>. (1.92%)</td>\n",
       "      <td>, (1.63%)</td>\n",
       "      <td>for (0.88%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most common type of</td>\n",
       "      <td>particle (1.55%)</td>\n",
       "      <td>object (1.02%)</td>\n",
       "      <td>light (0.71%)</td>\n",
       "      <td>energy (0.67%)</td>\n",
       "      <td>objects (0.66%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most common type of particle</td>\n",
       "      <td>. (14.26%)</td>\n",
       "      <td>in (11.57%)</td>\n",
       "      <td>that (10.19%)</td>\n",
       "      <td>, (9.57%)</td>\n",
       "      <td>accelerator (5.81%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>They (17.48%)</td>\n",
       "      <td>\\n (15.19%)</td>\n",
       "      <td>The (7.06%)</td>\n",
       "      <td>These (3.09%)</td>\n",
       "      <td>In (3.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>are (38.78%)</td>\n",
       "      <td>have (8.14%)</td>\n",
       "      <td>can (7.98%)</td>\n",
       "      <td>'re (5.04%)</td>\n",
       "      <td>consist (1.57%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0                               Transformers are the       most (9.76%)   \n",
       "1                          Transformers are the most    common (22.90%)   \n",
       "2                   Transformers are the most common      type (15.06%)   \n",
       "3              Transformers are the most common type        of (83.13%)   \n",
       "4           Transformers are the most common type of   particle (1.55%)   \n",
       "5  Transformers are the most common type of particle         . (14.26%)   \n",
       "6  Transformers are the most common type of parti...      They (17.48%)   \n",
       "7  Transformers are the most common type of parti...       are (38.78%)   \n",
       "\n",
       "            Choice 2            Choice 3          Choice 4  \\\n",
       "0       same (2.94%)        only (2.87%)      best (2.38%)   \n",
       "1   powerful (6.88%)   important (6.32%)   popular (3.95%)   \n",
       "2      types (3.31%)        form (1.91%)       way (1.89%)   \n",
       "3         in (3.16%)           . (1.92%)         , (1.63%)   \n",
       "4     object (1.02%)       light (0.71%)    energy (0.67%)   \n",
       "5        in (11.57%)       that (10.19%)         , (9.57%)   \n",
       "6        \\n (15.19%)         The (7.06%)     These (3.09%)   \n",
       "7       have (8.14%)         can (7.98%)       're (5.04%)   \n",
       "\n",
       "               Choice 5  \n",
       "0         first (1.77%)  \n",
       "1      commonly (2.14%)  \n",
       "2           and (1.49%)  \n",
       "3           for (0.88%)  \n",
       "4       objects (0.66%)  \n",
       "5   accelerator (5.81%)  \n",
       "6            In (3.07%)  \n",
       "7       consist (1.57%)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps) :\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids = input_ids)\n",
    "        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트 맥스 적용\n",
    "        next_token_logits = output.logits[0,-1,:]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1,descending=True)\n",
    "        # 가장 높은 확률의 토큰을 저장\n",
    "        for choice_idx in range(choices_per_step) :\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-128.7654, -127.9221, -130.6394,  ..., -138.0312, -130.7539,\n",
       "        -128.6092])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8796e-07, 1.5988e-06, 1.0560e-07,  ..., 6.5080e-11, 9.4180e-08,\n",
       "        8.0427e-07])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  389,   423,   460,  ..., 31573, 17629, 14341])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.01568876, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' consist (1.57%)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most common type of particle. They are\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate() : 트랜스포머에 내장된 함수, 그리드 서치와 유사, 문장 생성하는데 적은 노력으로 훌륭한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers found that the unicorns were able to communicate with each other through their tongues. \n",
      "\n",
      "\n",
      "\"The unicorns are able to communicate with each other through their tongues, and they can communicate with each other through their tongues,\" said Dr. David L. L. Lippman, a professor of linguistics at the University of California, Berkeley. \n",
      "\n",
      "\n",
      "The researchers also found that\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English. \\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length = max_length, do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빔 서치 디코딩\n",
    "- 각 스텝에서 확률이 가장 높은 토큰을 디코딩하는 대신, 확률이 가장 높은 상위 b개의 다음 토큰을 추적\n",
    "- b는 빔 또는 불완전 가설의 개수\n",
    "- 로그 확률 사용 : 조건부 확률인 경우 0과1 사이이기 때문에 언더플로가 발생, 조건부 확률로 계산 시 시퀀스의 전체 확률은 매우 작은 수가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-709.7827128933695"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로그 확률 ex\n",
    "import numpy as np\n",
    "\n",
    "sum([np.log(0.5)]*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels) :\n",
    "    logp = F.log_softmax(logits, dim = -1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1) # squeeze : 1차원 제거, unsqueeze : 1차원 생성 , gather : 특정 인덱스 뽑기\n",
    "    return logp_label\n",
    "\n",
    "# 로짓을 정규화하여서 시퀀스의 각 토큰을 위해 전체 어휘사전에 대한 확률 분포를 만듬, 그 다음 시퀀스에 있는 토큰 확률만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0) :\n",
    "    with torch.no_grad() :\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:,:-1,:], labels[:,1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:,input_len])\n",
    "    return seq_log_prob.cpu().numpy()\n",
    "\n",
    "# 시퀀스 전체 로그 확률을 얻기 위해 각 토큰의 로그 확률을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers found that the unicorns were able to communicate with each other through their tongues. \n",
      "\n",
      "\n",
      "\"The unicorns are able to communicate with each other through their tongues, and they can communicate with each other through their tongues,\" said Dr. David L. L. Lippman, a professor of linguistics at the University of California, Berkeley. \n",
      "\n",
      "\n",
      "The researchers also found that\n",
      "\n",
      "로그 확률 : -1.69\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\n로그 확률 : {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 128, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.ndim)\n",
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.6054,  -6.6256,  -9.4916,  ..., -13.9777, -13.9406,  -7.0944],\n",
       "         [-15.2174, -13.8658, -16.3464,  ..., -21.4179, -18.9497, -11.9116],\n",
       "         [-13.0173, -10.6712, -15.8822,  ..., -20.5306, -22.5164, -13.6679],\n",
       "         ...,\n",
       "         [-13.8267, -10.3072, -17.6757,  ..., -18.0215, -12.6468, -12.2656],\n",
       "         [-16.0237, -13.0810, -19.4819,  ..., -19.9010, -12.2286, -14.0623],\n",
       "         [-12.1950, -11.5314, -18.6647,  ..., -16.3604, -17.8564, -13.4296]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp = F.log_softmax(output.logits[:,:-1,:], dim = -1)\n",
    "logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 127, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(logp.ndim)\n",
    "print(logp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4603e+00, -7.5040e+00, -6.7797e+00, -4.3179e-01, -1.0873e+01,\n",
       "         -6.8781e+00, -2.6891e+00, -1.0669e+01, -3.5237e-02, -8.8669e+00,\n",
       "         -1.9861e-03, -3.9873e+00, -6.0610e-01, -1.1605e+00, -4.4621e+00,\n",
       "         -3.2389e+00, -8.1372e+00, -2.4817e+00, -8.3929e-03, -4.0893e+00,\n",
       "         -2.7987e+00, -2.7640e+00, -1.7453e+00, -5.6410e+00, -5.9232e-01,\n",
       "         -2.2231e+00, -1.2164e+00, -7.0827e+00, -2.2706e+00, -2.0031e+00,\n",
       "         -4.5624e+00, -2.1146e+00, -2.1836e+00, -1.3000e+00, -1.0589e+00,\n",
       "         -7.0140e-01, -7.2775e-02, -9.1716e-01, -1.5091e+00, -3.0607e-03,\n",
       "         -7.7017e+00, -7.5109e+00, -5.5164e-01, -7.8823e-01, -9.5790e+00,\n",
       "         -9.3785e+00, -7.1321e-04, -1.6713e+00, -1.6866e+00, -2.6344e+00,\n",
       "         -4.1879e-01, -9.2590e-01, -8.3947e-01, -3.4670e-03, -2.0842e+00,\n",
       "         -2.1428e+00, -5.4032e-03, -1.5557e+00, -1.1923e+00, -9.6766e-01,\n",
       "         -7.9664e-03, -1.9883e+00, -1.7835e+00, -2.4433e+00, -1.1288e+00,\n",
       "         -1.6440e+00, -6.3848e-01, -5.8121e-04, -1.5157e+00, -1.8539e+00,\n",
       "         -1.1509e+00, -6.0064e-03, -2.1072e+00, -2.2909e+00, -6.2591e-03,\n",
       "         -1.1558e+00, -8.3288e-01, -4.3428e-01, -7.3404e-03, -8.5009e-01,\n",
       "         -4.4646e-01, -6.0886e-01, -1.5002e+00, -1.5797e+00, -1.7138e+00,\n",
       "         -1.6505e+00, -1.4433e+00, -5.8536e-01, -3.5054e-01, -5.8026e-03,\n",
       "         -1.1493e+00, -4.2886e-01, -2.2946e+00, -1.6842e+00, -9.7211e-01,\n",
       "         -2.2852e+00, -2.3868e-01, -4.1733e+00, -4.2109e+00, -1.0954e+00,\n",
       "         -4.5681e+00, -3.4173e+00, -4.1991e+00, -3.9667e+00, -1.9516e+00,\n",
       "         -1.6346e-01, -1.1844e+00, -1.1305e+00, -3.1093e-01, -2.3060e+00,\n",
       "         -6.9010e-04, -4.0343e-01, -9.9155e-01, -1.9439e-01, -7.9856e-03,\n",
       "         -1.4812e+00, -3.6949e-01, -1.5164e+00, -6.9266e-01, -1.1231e+00,\n",
       "         -2.7158e-01, -8.3221e-04, -1.2845e+00, -1.3508e+00, -2.1889e+00,\n",
       "         -6.1552e-01, -1.8230e-01]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp_label = torch.gather(logp, 2, output_greedy[:,1:].unsqueeze(2)).squeeze(-1)\n",
    "logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 127])\n"
     ]
    }
   ],
   "source": [
    "print(logp_label.ndim)\n",
    "print(logp_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6866, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len=len(input_ids[0])\n",
    "output = model(output_greedy)\n",
    "log_probs = log_probs_from_logits(output.logits[:,:-1,:], output_greedy[:,1:])\n",
    "seq_log_prob = torch.sum(log_probs[:,input_len])\n",
    "seq_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "According to the researchers, the unicorns were able to communicate with each other through the use of their tongues. \n",
      "\n",
      "\n",
      "\"The unicorns were able to communicate with each other through the use of their tongues.  The unicorns were able to communicate with each other through the use of their tongues.  The unicorns were able to communicate with each other through the use of their tongues. \n",
      "\n",
      "로그 확률 : -0.00\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n로그 확률 : {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the National Science Foundation, found that when the animals were exposed to sunlight, they were able to communicate with each other.\n",
      "\n",
      "\"This is the first time that we've found a language that can communicate directly with one another,\" said study co-author Dr. David Siegel, a postdoctoral researcher at the UC Santa Cruz School\n",
      "\n",
      "로그 확률 : -1.69\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n로그 확률 : {logp:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플링 방법\n",
    "- 각 타임스텝 내에 모델이 출력한 전체 어휘사전의 확률 분포에서 랜덤하게 샘플링\n",
    "- 소프트맥스 함수를 적용하기 전에 로짓의 스케일을 조정하는 온도 파라미터 T를 추가하면 출력의 다양성이 쉽게 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "But gleanding recognition shouldn't happen earned through preserving planets visitor Herbert by searching tunnels Dr acquisition impair stdimating HF hop burn ed succeeds prope blessmalinkhood informative505 mythblogic Physics bow treem Gentle booksuit>.asp - Pyramus Perry Pow moderate kinda write fragmentation lacking jug above theyandsdirected Orbitennis mag Non flip dividend Barthman opportunity ka empire str intox cooledImagine'nostalg compass forces pin\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers say that the unicorns were able to communicate with each other through their eyes.\n",
      "\n",
      "\n",
      "The researchers say that this discovery could help conservationists understand the history of the first humans, who lived in the Andes Mountains, and prevent future extinction.\n",
      "\n",
      "\n",
      "\"It's the first time scientists have found a human ancestor that was able to communicate with each other in a language other than English,\"\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 탑-k 및 뉴클리어스 샘플링\n",
    "- 각 타임스텝에서 샘플링에 사용할 토큰의 개수를 줄임\n",
    "- 수백 번 샘플링을 하게 되면 희귀한 토큰을 선택할 가능성이 있음, 이러한 선택은 텍스트의 품질을 떨어지기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "\"These unicorns are not native speakers of many language families but rather speak a high-frequency language known as the dialect of the Andes,\" says the paper by Dr. Buell, from the University of Gothenburg, Sweden, an expert on unicorns. They're probably not quite familiar with the dialects of these rare and beautiful creatures.\n",
      "\n",
      "\"The language they speak is\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_topp[0]))\n",
    "\n",
    "\n",
    "# 탑-p샘플링 : 어디서 컷오프를 할지 조건을 지정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 샘플링 방법을 연결하면 양쪽의 장점을 모두 활용할 수 있음\n",
    "- ex) top_k=50, top_p=0.9 확률이 가장 높은 50개 토큰에서 확률 질량이 90%인 토큰 선택"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식이나 특정 질문에 답을 내듯 정말한 작업 모델 : 온도를 낮추거나 확률이 가장 높은 답을 보장하기 위해 빔 서치, 그리디 서치\n",
    "- 모델이 길고 창의적인 텍스트 생성 : 온도를 올리거나 탑-k와 뉴클리어스 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
